---
sidebar_label: Unterstützte Endpunkte
description: Informationen zu unterstützten API-Endpunkten und Beispiele
title: Unterstützte API-Endpunkte
---

Die API kann über HTTPS angesprochen werden und ist OpenAI-API-kompatibel. Die Base-URL lautet `https://llm.aihosting.mittwald.de`. Häufig wird in Applikationen gefordert, dass zusätzlich die Versionierung mit angegeben wird. In diesem Fall sollte die gesamte Base-URL mit Versionierung angegeben werden: `https://llm.aihosting.mittwald.de/v1`.

Jede Interaktion mit der API erfordert, dass ein `Authorization`-Header mit einem gültigen API-Key übermittelt wird. Dieser Key kann im mStudio erstellt werden.

Wir verwenden in unseren Beispielen `curl`, da dies die einfachste und schnellste Variante zum Testen ist. Für einen produktiven Einsatz empfehlen wir den Einsatz von Frameworks und Libraries, die OpenAI unterstützen. Um die nachfolgenden Beispiele erfolgreich auszuführen, muss der API-Key zunächst als Umgebungsvariable gesetzt werden, beispielsweise so:

```sh
export APIKEY=sk-…
```

## /v1/models

Dieser Endpunkt gibt eine Liste der zur Verfügung stehenden Modelle zurück.

```sh
curl -i -X GET https://llm.aihosting.mittwald.de/v1/models \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY"
```

Es wird ein Dictionary mit einer Liste von verfügbaren Modellen zurückgeliefert. Die darin befindliche `id` kann für die nachfolgenden API-Routen im `model`-Feld verwendet werden.

## /v1/chat/completions und /v1/completions

Über diese Route können Inhalte an das LLM im Chat-Format übersendet werden. Der Endpunkt unterstützt Streaming für die vom LLM generierten Inhalte.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ]
    }'

```

Der Parameter `model` erfordert eine gültige Modellbezeichnung, welcher über die `/v1/models`-Route ermittelt werden kann. Es können weitere Modellparameter wie `temperature`, `top_p` oder `top_k` übermittelt werden. Empfohlene Einstellungen hierfür finden sich in der Beschreibung der Modelle, sofern diese vom Standard abweichen. Welche erweiterten Parameter Einfluss auf die Antwort nehmen ist modellabhängig.

Für eine Stream-Response muss die Option `stream: true` gesetzt werden.

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ],
        "stream": true,
        "temperature": 0.15,
        "top_k": 10,
        "top_p": 0.5
    }'
```

### Vision (Bild + Text)

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [{
          "role": "user",
          "content": [
            {"type": "text", "text": "Was ist auf diesem Bild?"},
            {"type": "image_url", "image_url": {"url": "https://example.com/photo.jpg"}}
          ]
        }],
        "temperature": 0.1
    }'
```

### Function Calling (Tools)

Das Modell kann Funktionen/Tools aufrufen, die du definierst. Die API gibt den Funktionsnamen und die Argumente zurück, die aufgerufen werden sollen - du musst die eigentliche Funktion selbst implementieren und ausführen.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
      "model": "Devstral-Small-2-24B-Instruct-2512",
      "messages": [{"role": "user", "content": "Wetter in Berlin?"}],
      "tools": [{
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Aktuelles Wetter abrufen",
          "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string"}},
            "required": ["city"]
          }
        }
      }],
      "tool_choice": "auto"
    }'
```

**Wichtig**: Die Funktion `get_weather` (oder jedes andere Tool, das du definierst) muss in deiner Anwendung implementiert werden. Die API teilt dir nur mit, welche Funktion mit welchen Argumenten aufgerufen werden soll - du bist dafür verantwortlich, die Funktion auszuführen und die Ergebnisse bei Bedarf an das Modell zurückzugeben.

## /v1/responses (experimentell)

Dieser Endpunkt bietet Zugriff auf die **Responses API**, welcher auf der Struktur von der [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) basiert.
Da die Route noch **experimentell** ist, sind nicht alle Funktionen vollständig verfügbar. Einige erweiterte Parameter können sich anders verhalten als in der [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).

Strukturierte Ausgaben wie schema-basiertes JSON können über diesen Endpunkt derzeit nicht erzwungen werden.
Falls ein festes Ausgabeformat benötigt wird, sollte stattdessen der Endpunkt /v1/chat/completions verwendet werden

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/responses \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "input": "Erkläre den Zweck dieses experimentellen Endpunkts."
    }'
```

Die Antwort enthält den generierten Text sowie Metadaten.
Da der Endpunkt noch in Entwicklung ist, können einzelne Felder fehlen oder abweichend umgesetzt sein.

Um eine gestreamte Antwort zu erhalten, wird `stream: true` gesetzt:

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/responses \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "input": "Sende eine kurze gestreamte Nachricht.",
        "stream": true,
        "temperature": 0.2,
        "top_p": 0.5,
        "top_k": 20
    }'
```

Streaming wird unterstützt, kann jedoch je nach Funktion oder Parameter leicht eingeschränkt sein.

## /v1/embeddings

Mit dieser Route können Embeddings für Texte erstellt werden.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/embeddings \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Qwen3-Embedding-8B",
        "input": "Ein wichtiges Dokument"
    }'
```

Basierend auf dem verwendeten Embedding-Modell können weitere vom Modell unterstützte Parameter wie `dimensions` übermittelt werden. Dies ist bei [Qwen3-Embedding-8B](../../models/qwen3-embedding-8b/) jedoch nicht der Fall.

## /v1/audio/transcriptions

Dieser Endpunkt ermöglicht es, Audiodateien mit Whisper-Modellen in Text zu transkribieren.

```sh
curl -X POST https://llm.aihosting.mittwald.de/v1/audio/transcriptions \
    -H "Authorization: Bearer $APIKEY" \
    -F "file=@/path/to/audio.mp3" \
    -F "model=Whisper-Large-V3-Turbo"
```

Der `file`-Parameter sollte die zu transkribierende Audiodatei enthalten. Unterstützte Formate sind MP3, OGG, WAV und FLAC. Die maximale Dateigröße beträgt 25 MB. Der `model`-Parameter gibt an, welches Whisper-Modell für die Transkription verwendet werden soll.

Zusätzlich können folgende optionale Parameter übermittelt werden:

- `language`: Die Sprache der Audioeingabe (ISO-639-1-Format). Falls nicht angegeben, wird standardmäßig Deutsch ("de") angenommen. Es wird dringend empfohlen, diesen Parameter immer explizit zu setzen, um die beste Genauigkeit zu erzielen.
- `temperature`: Sampling-Temperatur, empfohlener Wert ist 1.0
- `response_format`: Format der Antwort (json oder text). Andere Formate wie srt, vtt und verbose_json werden derzeit nicht unterstützt.

```sh
curl -X POST https://llm.aihosting.mittwald.de/v1/audio/transcriptions \
    -H "Authorization: Bearer $APIKEY" \
    -F "file=@/path/to/audio.mp3" \
    -F "model=Whisper-Large-V3-Turbo" \
    -F "language=de" \
    -F "temperature=1.0" \
    -F "response_format=json"
```

**Wichtige Hinweise:**

- Übersetzungsfunktion (`to_language`-Parameter) wird nicht unterstützt
- Für beste Ergebnisse sollte der `language`-Parameter immer explizit angegeben werden
- Segmentiere große Audiodateien bei Bedarf in Teile kleiner als 25 MB
