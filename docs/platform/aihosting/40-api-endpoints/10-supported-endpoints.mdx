---
sidebar_label: Supported endpoints
description: Supported API endpoints and usage examples
title: Supported API endpoints
---

The API is accessible via HTTPS and is compatible with the OpenAI API. The base URL is `https://llm.aihosting.mittwald.de`. Many applications require specifying the version as well. In this case, the full base URL with versioning should be used: `https://llm.aihosting.mittwald.de/v1`.

Every interaction with the API requires an `Authorization` header with a valid API key. This key can be created in mStudio.

In our examples, we use `curl`, as it is the simplest and quickest way to test. For production use, we recommend using frameworks and libraries that support OpenAI. To run the following examples please setup your API key as an environment variable first, e.g.

```sh
export APIKEY=sk-â€¦
```

## /v1/models

This endpoint returns a list of available models.

```sh
curl -i -X GET https://llm.aihosting.mittwald.de/v1/models \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY"
```

It returns a dictionary containing a list of available models. The `id` within this list can be used in the `model` field for subsequent API routes.

## /v1/chat/completions and /v1/completions

This route allows content to be sent to the LLM in chat format. The endpoint supports streaming for the content generated by the LLM.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [
            {
                "role": "user",
                "content": "Moin and hello!"
            }
        ]
    }'

```

The `model` parameter requires a valid model name, which can be retrieved via the `/v1/models` route. Additional model parameters such as `temperature`, `top_p`, or `top_k` can be provided. Recommended settings for these can be found in the model descriptions if they differ from the defaults. Which extended parameters influence the response depends on the model.

To receive a streamed response, the option `stream: true` must be set.

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [
            {
                "role": "user",
                "content": "Moin and hello!"
            }
        ],
        "stream": true,
        "temperature": 0.15,
        "top_k": 10,
        "top_p": 0.5
    }'
```

### Vision (image + text)

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "messages": [{
          "role": "user",
          "content": [
            {"type": "text", "text": "What is in this image?"},
            {"type": "image_url", "image_url": {"url": "https://example.com/photo.jpg"}}
          ]
        }],
        "temperature": 0.1
    }'
```

### Function calling (tools)

The model can call functions/tools that you define. The API will return the function name and arguments that should be called - you must implement and execute the actual function yourself.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
      "model": "Devstral-Small-2-24B-Instruct-2512",
      "messages": [{"role": "user", "content": "Weather in Berlin?"}],
      "tools": [{
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get current weather",
          "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string"}},
            "required": ["city"]
          }
        }
      }],
      "tool_choice": "auto"
    }'
```

**Important**: The `get_weather` function (or any tool you define) must be implemented in your application. The API only tells you which function to call and with what arguments - you are responsible for executing the function and returning the results back to the model if needed.

## /v1/responses (experimental)

This endpoint provides access to the **Responses API**, following the structure of the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).
As the route is still **experimental**, not all features are fully supported. Some extended parameters may behave differently compared to the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).

Structured output such as schema-based JSON **cannot currently be enforced** with this endpoint.
If structured output is required, the `/v1/chat/completions` endpoint should be used instead.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/responses \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "input": "Explain the purpose of this experimental endpoint."
    }'
```

The response contains the generated text and metadata.
Since the endpoint is still under development, some fields may be missing or implemented differently.

To receive a streamed response, set `stream: true`:

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/responses \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Ministral-3-14B-Instruct-2512",
        "input": "Send a short streamed message.",
        "stream": true,
        "temperature": 0.2,
        "top_p": 0.5,
        "top_k": 20
    }'
```

Streaming is supported, but certain functions or parameters may be limited due to the experimental status of the endpoint.

## /v1/embeddings

This route allows you to generate embeddings for texts.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/embeddings \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{
        "model": "Qwen3-Embedding-8B",
        "input": "An important document"
    }'
```

Depending on the embedding model used, additional supported parameters such as `dimensions` can be submitted. However, this is not applicable to [Qwen3-Embedding-8B](../../models/qwen3-embedding-8b/).

## /v1/audio/transcriptions

This endpoint allows you to transcribe audio files into text using Whisper models.

```sh
curl -X POST https://llm.aihosting.mittwald.de/v1/audio/transcriptions \
    -H "Authorization: Bearer $APIKEY" \
    -F "file=@/path/to/audio.mp3" \
    -F "model=Whisper-Large-V3-Turbo"
```

The `file` parameter should contain the audio file to be transcribed. Supported formats include MP3, OGG, WAV, and FLAC. Maximum file size is 25 MB. The `model` parameter specifies which Whisper model to use for transcription.

Additional optional parameters can be provided:

- `language`: The language of the input audio (ISO-639-1 format). If not specified, German ("de") will be assumed by default. It is strongly recommended to always set this parameter explicitly for best accuracy.
- `temperature`: Sampling temperature, recommended value is 1.0
- `response_format`: Format of the response (json or text). Other formats like srt, vtt, and verbose_json are not currently supported.

```sh
curl -X POST https://llm.aihosting.mittwald.de/v1/audio/transcriptions \
    -H "Authorization: Bearer $APIKEY" \
    -F "file=@/path/to/audio.mp3" \
    -F "model=Whisper-Large-V3-Turbo" \
    -F "language=de" \
    -F "temperature=1.0" \
    -F "response_format=json"
```

**Important notes:**

- Translation functionality (`to_language` parameter) is not supported
- For best results, always specify the `language` parameter explicitly
- Segment large audio files into chunks smaller than 25 MB if needed
